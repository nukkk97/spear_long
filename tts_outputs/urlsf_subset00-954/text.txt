How transferable are features in deep neural networks Part of Advances in Neural Information Processing Systems Authors Conference Event Type Oral Abstract Many deep neural networks trained on natural images exhibit curious phenomenon in common on the first layer they learn features similar to Gabor filters and color blobs Such first layer features appear not to be specific to particular dataset or task but general in that they are applicable to many datasets and tasks Features must eventually transition from general to specific by the last layer of the network but this transition has not been studied extensively In this paper we experimentally quantify the generality versus specificity of neurons in each layer of deep convolutional neural network and report few surprising results Transferability is negatively affected by two distinct issues the specialization of higher layer neurons to their original task at the expense of performance on the target task which was expected and optimization difficulties related to splitting networks between co adapted neurons which was not expected In an example network trained on ImageNet we demonstrate that either 
